{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10807452,"sourceType":"datasetVersion","datasetId":6708542}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Assignment: Image recognition\n- Alumno 1: Nicoleta Roman\n- Alumno 2: Esteban Aspe\n- Alumno 3: Jelena Duric\n\nThe goals of the assignment are:\n* Develop proficiency in using Tensorflow/Keras for training Neural Nets (NNs).\n* Put into practice the acquired knowledge to optimize the parameters and architecture of a feedforward Neural Net (ffNN), in the context of an image recognition problem.\n* Put into practice NNs specially conceived for analysing images. Design and optimize the parameters of a Convolutional Neural Net (CNN) to deal with previous task.\n* Train popular architectures from scratch (e.g., GoogLeNet, VGG, ResNet, ...), and compare the results with the ones provided by their pre-trained versions using transfer learning.\n\nFollow the link below to download the classification data set  “xview_recognition”: [https://drive.upm.es/s/4oNHlRFEd71HXp4](https://drive.upm.es/s/4oNHlRFEd71HXp4)","metadata":{"editable":true,"id":"QYuALZOG-AMq","slideshow":{"slide_type":""},"tags":[]}},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.config.list_physical_devices('GPU'))","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import uuid\nimport numpy as np\n\nclass GenericObject:\n    \"\"\"\n    Generic object data.\n    \"\"\"\n    def __init__(self):\n        self.id = uuid.uuid4()\n        self.bb = (-1, -1, -1, -1)\n        self.category= -1\n        self.score = -1\n\nclass GenericImage:\n    \"\"\"\n    Generic image data.\n    \"\"\"\n    def __init__(self, filename):\n        self.filename = filename\n        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n        self.objects = list([])\n\n    def add_object(self, obj: GenericObject):\n        self.objects.append(obj)","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categories = {0: 'Cargo plane', 1: 'Helicopter', 2: 'Small car', 3: 'Bus', 4: 'Truck', 5: 'Motorboat', 6: 'Fishing vessel', 7: 'Dump truck', 8: 'Excavator', 9: 'Building', 10: 'Storage tank', 11: 'Shipping container'}","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rasterio","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nimport rasterio\nimport numpy as np\n\ndef load_geoimage(filename):\n    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n    src_raster = rasterio.open('/kaggle/input/deep-learning/xview_recognition/'+filename, 'r')\n    # RasterIO to OpenCV (see inconsistencies between libjpeg and libjpeg-turbo)\n    input_type = src_raster.profile['dtype']\n    input_channels = src_raster.count\n    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n    for band in range(input_channels):\n        img[:, :, band] = src_raster.read(band+1)\n    return img","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Training\nDesign and train a CNN to deal with the “xview_recognition” classification task.","metadata":{"id":"diNBB3qy-AM2"}},{"cell_type":"code","source":"import json\n# Load database\n\njson_file = '/kaggle/input/deep-learning/xview_recognition/xview_ann_train.json' \nwith open(json_file) as ifs:\n    json_data = json.load(ifs)\nifs.close()","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ncounts = dict.fromkeys(categories.values(), 0)\nanns = []\nfor json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n    image = GenericImage(json_img['filename'])\n    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n    obj = GenericObject()\n    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n    obj.category = json_ann['category_id']\n    # Resampling strategy to reduce training time\n    counts[obj.category] += 1\n    image.add_object(obj)\n    anns.append(image)\nprint(counts)","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nlabels = [image.objects[0].category for image in anns]\nanns_train, anns_valid = train_test_split(anns, test_size=0.12, random_state=1, shuffle=True, stratify=labels)\nprint('Number of training images: ' + str(len(anns_train)))\nprint('Number of validation images: ' + str(len(anns_valid)))","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load architecture\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LeakyReLU, GlobalAveragePooling2D, Dropout,BatchNormalization,GlobalMaxPooling2D, Concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.regularizers import l2\nprint('Load model')\n\ndef build_model():\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3), name=\"resnet50_base\")\n    \n    # Freeze convolutional layers\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    # Add a custom classifier head with Leaky ReLU\n    x = base_model.output\n    avg = GlobalAveragePooling2D()(x) \n    maxp = GlobalMaxPooling2D()(x) \n    x = Concatenate()([avg, maxp]) \n    x = BatchNormalization()(x) \n    x = Dense(512, kernel_regularizer=l2(1e-4))(x) \n    x = LeakyReLU(alpha=0.01)(x) \n    x = Dropout(0.5)(x) \n    x = Dense(12, activation=\"softmax\")(x) \n\n    \n    # Ensure all outputs are valid Keras tensors\n    model = Model(inputs=base_model.input, outputs=x)\n\n    # Optimizer settings\n    opt = Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\nstrategy = tf.distribute.MirroredStrategy() # Strategy for multi gpu usage\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n# buiding a model inside the strategy scope\nwith strategy.scope():\n    original_model = build_model()\n    multi_gpu_model = original_model\n    \nmulti_gpu_model.summary()","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras import backend as K\n\nclass ProgressiveUnfreezeCallback(Callback):\n    def __init__(self, unfreeze_epoch, n_layers_to_unfreeze, new_learning_rate=1e-4):\n        super().__init__()\n        self.unfreeze_epoch = unfreeze_epoch\n        self.n_layers_to_unfreeze = n_layers_to_unfreeze\n        self.new_learning_rate = new_learning_rate\n        self.unfreezing_done = False\n        # Hard-coded index for the last layer of the base model (ResNet50's \"conv5_block3_out\")\n        self.base_end_index = 174\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if (epoch == self.unfreeze_epoch) and (not self.unfreezing_done):\n            start_index = max(0, self.base_end_index - self.n_layers_to_unfreeze + 1)\n            print(f\"Unfreezing layers from index {start_index} to {self.base_end_index} (total {self.n_layers_to_unfreeze} layers)...\")\n            for layer in self.model.layers[start_index:self.base_end_index + 1]:\n                layer.trainable = True\n\n            # Update the learning rate similar to ReduceLROnPlateau:\n            self.model.optimizer.learning_rate = self.new_learning_rate\n            print(f\"Epoch {epoch}: Updated learning rate to {self.new_learning_rate} and unfroze {self.n_layers_to_unfreeze} layers.\")\n            self.unfreezing_done = True\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\n# Callbacks\nprogressive_unfreeze_cb = ProgressiveUnfreezeCallback(unfreeze_epoch=5, n_layers_to_unfreeze=10, new_learning_rate=1e-4)\nmodel_checkpoint = ModelCheckpoint('model.keras', monitor='val_accuracy', verbose=1, save_best_only=True)\nreduce_lr = ReduceLROnPlateau('val_accuracy', factor=0.1, patience=10, verbose=1)\nearly_stop = EarlyStopping('val_accuracy', patience=40, verbose=1)\nterminate = TerminateOnNaN()\ncallbacks = [model_checkpoint, reduce_lr, early_stop, terminate,progressive_unfreeze_cb]","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\ndatagen = ImageDataGenerator(\n    rotation_range=20,         # Rotate images by up to 20 degrees\n    width_shift_range=0.2,     # Shift width by 20%\n    height_shift_range=0.2,    # Shift height by 20%\n    shear_range=0.2,           # Shear transformation\n    zoom_range=0.2,            # Random zoom\n    horizontal_flip=True,      # Flip images horizontally\n    fill_mode='nearest'        # Fill missing pixels\n)\n\n\n\n\ndef generator_images(objs, batch_size, do_shuffle=False, data_augm=False):\n    while True:\n        if do_shuffle:\n            np.random.shuffle(objs)\n\n        groups = [objs[i:i+batch_size] for i in range(0, len(objs), batch_size)]\n        for group in groups:\n            images, labels = [], []\n            for (filename, obj) in group:\n                # Load image\n                img = load_geoimage(filename)  \n                \n                images.append(img)\n                probabilities = np.zeros(len(categories))\n                probabilities[list(categories.values()).index(obj.category)] = 1\n                labels.append(probabilities)\n            \n            images = np.array(images).astype(np.float32)\n            labels = np.array(labels).astype(np.float32)\n\n            # Apply augmentation only for training\n            if data_augm:\n                aug_generator = datagen.flow(images, labels, batch_size=batch_size, shuffle=True)\n                yield next(aug_generator)  # Return augmented batch\n            else:\n                yield images, labels  # No augmentation for validation\n","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate the list of objects from annotations\nobjs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\nobjs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n# Generators\nbatch_size = 128\ntrain_generator = generator_images(objs_train, batch_size, do_shuffle=True, data_augm=True)\nvalid_generator = generator_images(objs_valid, batch_size, do_shuffle=False)","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\nimport numpy as np\n\nprint('Training model')\nepochs = 70\ntrain_steps = math.ceil(len(objs_train)/batch_size)\nvalid_steps = math.ceil(len(objs_valid)/batch_size)\n\nwith tf.device(\"/device:GPU:0\"):\n    h = multi_gpu_model.fit(train_generator, steps_per_epoch=train_steps, validation_data=valid_generator, validation_steps=valid_steps, epochs=epochs, callbacks=callbacks, verbose=1)\n# Best validation model\nbest_idx = int(np.argmax(h.history['val_accuracy']))\nbest_value = np.max(h.history['val_accuracy'])\nprint('Best validation model: epoch ' + str(best_idx+1), ' - val_accuracy ' + str(best_value))","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Validation\nCompute validation metrics.","metadata":{"editable":true,"id":"8IMMO_mT-AM9","slideshow":{"slide_type":""},"tags":[]}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef draw_confusion_matrix(cm, categories):\n    # Draw confusion matrix\n    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n    ax = fig.add_subplot(111)\n    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap('Blues'))\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=list(categories.values()), yticklabels=list(categories.values()), ylabel='Annotation', xlabel='Prediction')\n    # Rotate the tick labels and set their alignment\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    # Loop over data dimensions and create text annotations\n    thresh = cm.max() / 2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n    fig.tight_layout()\n    plt.savefig(\"/kaggle/working/confusion_matrix.png\", format=\"png\", bbox_inches=\"tight\")\n    plt.close()","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nmodel = multi_gpu_model\nmulti_gpu_model.load_weights('model.keras')\ny_true, y_pred = [], []\nfor ann in anns_valid:\n    # Load image\n    image = load_geoimage(ann.filename)\n    for obj_pred in ann.objects:\n        # Generate prediction\n        warped_image = np.expand_dims(image, 0)\n        predictions = model.predict(warped_image, verbose=0)\n        # Save prediction\n        pred_category = list(categories.values())[np.argmax(predictions)]\n        pred_score = np.max(predictions)\n        y_true.append(obj_pred.category)\n        y_pred.append(pred_category)","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))\ndraw_confusion_matrix(cm, categories)\n\nresults = h.history\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 10))\n\n# Plot Accuracy\naxes[0].plot(results[\"accuracy\"], label=\"Training Accuracy\", color=\"blue\")\naxes[0].plot(results[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"green\")\naxes[0].set_ylabel(\"Accuracy\")\naxes[0].set_title(\"Accuracy Over Epochs\")\naxes[0].legend()\naxes[0].grid(True)\naxes[0].set_ylim(0, 1)  # Normalize y-axis for accuracy (0-1 range)\n\n# Plot Loss\naxes[1].plot(results[\"loss\"], label=\"Training Loss\", color=\"red\")\naxes[1].plot(results[\"val_loss\"], label=\"Validation Loss\", color=\"orange\")\naxes[1].set_ylabel(\"Loss\")\naxes[1].set_title(\"Loss Over Epochs\")\naxes[1].legend()\naxes[1].grid(True)\naxes[1].set_ylim(0, max(max(results[\"loss\"]), max(results[\"val_loss\"])) * 1.1)  # Normalize loss y-axis\n\nplt.xlabel(\"Epochs\")\nplt.tight_layout()\n\nplt.savefig(\"/kaggle/working/plot.png\", format=\"png\", bbox_inches=\"tight\")\nplt.close()","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Compute the accuracy\ncorrect_samples_class = np.diag(cm).astype(float)\ntotal_samples_class = np.sum(cm, axis=1).astype(float)\ntotal_predicts_class = np.sum(cm, axis=0).astype(float)\nprint('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\nacc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\nprint('Mean Recall: %.3f%%' % (acc.mean() * 100))\nacc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\nprint('Mean Precision: %.3f%%' % (acc.mean() * 100))\nfor idx in range(len(categories)):\n    # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n    # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n    tp = cm[idx, idx]\n    fp = sum(cm[:, idx]) - tp\n    fn = sum(cm[idx, :]) - tp\n    tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n    # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n    recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n    # Precision: proportion of predicted positive cases that were truly real positives.\n    precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n    # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n    specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n    # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n    # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n    f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n    print('> %s: Recall: %.3f%% Precision: %.3f%% Specificity: %.3f%% Dice: %.3f%%' % (list(categories.values())[idx], recall*100, precision*100, specificity*100, f1_score*100))","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Testing\nTry to improve the results provided in the competition.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\n\nanns = []\nfor (dirpath, dirnames, filenames) in os.walk('/kaggle/input/deep-learning/xview_recognition/xview_test'):\n#for (dirpath, dirnames, filenames) in os.walk('xview_recognition/xview_test'):\n    \n    for filename in filenames:\n        #image = GenericImage(dirpath[18:]+'/'+filename)\n        image = GenericImage('xview_test/'+filename)\n        image.tile = np.array([0, 0, 224, 224])\n        obj = GenericObject()\n        obj.bb = (0, 0, 224, 224)\n        obj.category = dirpath[dirpath.rfind('/')+1:]\n        image.add_object(obj)\n        anns.append(image)\nprint('Number of testing images: ' + str(len(anns)))","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nmodel.load_weights('model.keras')\npredictions_data = {\"images\": {}, \"annotations\": {}}\nfor idx, ann in enumerate(anns):\n    image_data = {\"image_id\": ann.filename.split('/')[-1], \"filename\": ann.filename, \"width\": int(ann.tile[2]), \"height\": int(ann.tile[3])}\n    predictions_data[\"images\"][idx] = image_data\n    # Load image\n    image = load_geoimage(ann.filename)\n    for obj_pred in ann.objects:\n        # Generate prediction\n        warped_image = np.expand_dims(image, 0)\n        predictions = model.predict(warped_image, verbose=0)\n        # Save prediction\n        pred_category = list(categories.values())[np.argmax(predictions)]\n        pred_score = np.max(predictions)\n        annotation_data = {\"image_id\": ann.filename.split('/')[-1], \"category_id\": pred_category, \"bbox\": [int(x) for x in obj_pred.bb]}\n        predictions_data[\"annotations\"][idx] = annotation_data","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"/kaggle/working/prediction.json\", \"w\") as outfile:\n    json.dump(predictions_data, outfile)\n#with open(\"prediction.json\", \"w\") as outfile:\n    #json.dump(predictions_data, outfile)","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"execution_count":null}]}