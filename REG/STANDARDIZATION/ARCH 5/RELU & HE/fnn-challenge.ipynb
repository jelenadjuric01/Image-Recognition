{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c8ce80",
   "metadata": {
    "editable": true,
    "id": "QYuALZOG-AMq",
    "papermill": {
     "duration": 0.005165,
     "end_time": "2025-02-27T16:47:04.717551",
     "exception": false,
     "start_time": "2025-02-27T16:47:04.712386",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Assignment: Image recognition\n",
    "- Alumno 1: Nicoleta Roman\n",
    "- Alumno 2: Esteban Aspe\n",
    "- Alumno 3: Jelena Duric\n",
    "\n",
    "The goals of the assignment are:\n",
    "* Develop proficiency in using Tensorflow/Keras for training Neural Nets (NNs).\n",
    "* Put into practice the acquired knowledge to optimize the parameters and architecture of a feedforward Neural Net (ffNN), in the context of an image recognition problem.\n",
    "* Put into practice NNs specially conceived for analysing images. Design and optimize the parameters of a Convolutional Neural Net (CNN) to deal with previous task.\n",
    "* Train popular architectures from scratch (e.g., GoogLeNet, VGG, ResNet, ...), and compare the results with the ones provided by their pre-trained versions using transfer learning.\n",
    "\n",
    "Follow the link below to download the classification data set  “xview_recognition”: [https://drive.upm.es/s/4oNHlRFEd71HXp4](https://drive.upm.es/s/4oNHlRFEd71HXp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d1db6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.031186Z",
     "start_time": "2024-10-26T00:00:17.131476Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:04.727606Z",
     "iopub.status.busy": "2025-02-27T16:47:04.727331Z",
     "iopub.status.idle": "2025-02-27T16:47:17.551787Z",
     "shell.execute_reply": "2025-02-27T16:47:17.550765Z"
    },
    "papermill": {
     "duration": 12.830964,
     "end_time": "2025-02-27T16:47:17.553210",
     "exception": false,
     "start_time": "2025-02-27T16:47:04.722246",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0feb1581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.066937Z",
     "start_time": "2024-10-26T00:00:21.059126Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:17.564089Z",
     "iopub.status.busy": "2025-02-27T16:47:17.563573Z",
     "iopub.status.idle": "2025-02-27T16:47:17.569094Z",
     "shell.execute_reply": "2025-02-27T16:47:17.568313Z"
    },
    "id": "OYtqD3Oh-AMw",
    "papermill": {
     "duration": 0.012155,
     "end_time": "2025-02-27T16:47:17.570366",
     "exception": false,
     "start_time": "2025-02-27T16:47:17.558211",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "class GenericObject:\n",
    "    \"\"\"\n",
    "    Generic object data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.bb = (-1, -1, -1, -1)\n",
    "        self.category= -1\n",
    "        self.score = -1\n",
    "\n",
    "class GenericImage:\n",
    "    \"\"\"\n",
    "    Generic image data.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n",
    "        self.objects = list([])\n",
    "\n",
    "    def add_object(self, obj: GenericObject):\n",
    "        self.objects.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70cd053f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.153693Z",
     "start_time": "2024-10-26T00:00:21.149079Z"
    },
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:17.580277Z",
     "iopub.status.busy": "2025-02-27T16:47:17.580043Z",
     "iopub.status.idle": "2025-02-27T16:47:17.583669Z",
     "shell.execute_reply": "2025-02-27T16:47:17.582861Z"
    },
    "id": "I_GygShu-AMz",
    "papermill": {
     "duration": 0.010216,
     "end_time": "2025-02-27T16:47:17.585191",
     "exception": false,
     "start_time": "2025-02-27T16:47:17.574975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = {0: 'Cargo plane', 1: 'Helicopter', 2: 'Small car', 3: 'Bus', 4: 'Truck', 5: 'Motorboat', 6: 'Fishing vessel', 7: 'Dump truck', 8: 'Excavator', 9: 'Building', 10: 'Storage tank', 11: 'Shipping container'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9951a423",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:17.603052Z",
     "iopub.status.busy": "2025-02-27T16:47:17.602814Z",
     "iopub.status.idle": "2025-02-27T16:47:23.297307Z",
     "shell.execute_reply": "2025-02-27T16:47:23.296148Z"
    },
    "papermill": {
     "duration": 5.705163,
     "end_time": "2025-02-27T16:47:23.299099",
     "exception": false,
     "start_time": "2025-02-27T16:47:17.593936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rasterio\r\n",
      "  Downloading rasterio-1.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\r\n",
      "Collecting affine (from rasterio)\r\n",
      "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\r\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (25.1.0)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2025.1.31)\r\n",
      "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\r\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\r\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.26.4)\r\n",
      "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from rasterio) (3.2.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->rasterio) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->rasterio) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->rasterio) (2024.2.0)\r\n",
      "Downloading rasterio-1.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\r\n",
      "Installing collected packages: affine, rasterio\r\n",
      "Successfully installed affine-2.4.0 rasterio-1.4.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d56d91d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.292654Z",
     "start_time": "2024-10-26T00:00:21.205321Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:23.311665Z",
     "iopub.status.busy": "2025-02-27T16:47:23.311385Z",
     "iopub.status.idle": "2025-02-27T16:47:23.628090Z",
     "shell.execute_reply": "2025-02-27T16:47:23.627200Z"
    },
    "id": "fRBA7ReQ-AM0",
    "papermill": {
     "duration": 0.324579,
     "end_time": "2025-02-27T16:47:23.629684",
     "exception": false,
     "start_time": "2025-02-27T16:47:23.305105",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "def load_geoimage(filename):\n",
    "    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "    src_raster = rasterio.open('/kaggle/input/deeplearning/xview_recognition/'+filename, 'r')\n",
    "    # RasterIO to OpenCV (see inconsistencies between libjpeg and libjpeg-turbo)\n",
    "    input_type = src_raster.profile['dtype']\n",
    "    input_channels = src_raster.count\n",
    "    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n",
    "    for band in range(input_channels):\n",
    "        img[:, :, band] = src_raster.read(band+1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3307dd7f",
   "metadata": {
    "id": "diNBB3qy-AM2",
    "papermill": {
     "duration": 0.005178,
     "end_time": "2025-02-27T16:47:23.640542",
     "exception": false,
     "start_time": "2025-02-27T16:47:23.635364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Training\n",
    "Design and train a ffNN to deal with the “xview_recognition” classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fea6f1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.416449Z",
     "start_time": "2024-10-26T00:00:21.311510Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:23.651821Z",
     "iopub.status.busy": "2025-02-27T16:47:23.651415Z",
     "iopub.status.idle": "2025-02-27T16:47:23.813549Z",
     "shell.execute_reply": "2025-02-27T16:47:23.812844Z"
    },
    "id": "Orto292C-AM3",
    "papermill": {
     "duration": 0.169458,
     "end_time": "2025-02-27T16:47:23.815165",
     "exception": false,
     "start_time": "2025-02-27T16:47:23.645707",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Load database\n",
    "\n",
    "json_file = '/kaggle/input/deeplearning/xview_recognition/xview_ann_train.json' \n",
    "with open(json_file) as ifs:\n",
    "    json_data = json.load(ifs)\n",
    "ifs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb92ea30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:22.874518Z",
     "start_time": "2024-10-26T00:00:22.204948Z"
    },
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:23.827117Z",
     "iopub.status.busy": "2025-02-27T16:47:23.826811Z",
     "iopub.status.idle": "2025-02-27T16:47:24.198280Z",
     "shell.execute_reply": "2025-02-27T16:47:24.197209Z"
    },
    "id": "4GjFLHs4-AM4",
    "outputId": "5581df22-d4e9-42ac-9f94-061fd8c7acd9",
    "papermill": {
     "duration": 0.378892,
     "end_time": "2025-02-27T16:47:24.199680",
     "exception": false,
     "start_time": "2025-02-27T16:47:23.820788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cargo plane': 635, 'Helicopter': 70, 'Small car': 4290, 'Bus': 2155, 'Truck': 2746, 'Motorboat': 1069, 'Fishing vessel': 706, 'Dump truck': 1236, 'Excavator': 789, 'Building': 4689, 'Storage tank': 1469, 'Shipping container': 1523}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "counts = dict.fromkeys(categories.values(), 0)\n",
    "anns = []\n",
    "for json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n",
    "    image = GenericImage(json_img['filename'])\n",
    "    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n",
    "    obj = GenericObject()\n",
    "    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n",
    "    obj.category = json_ann['category_id']\n",
    "    # Resampling strategy to reduce training time\n",
    "    counts[obj.category] += 1\n",
    "    image.add_object(obj)\n",
    "    anns.append(image)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0790b5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:23.656800Z",
     "start_time": "2024-10-26T00:00:23.123245Z"
    },
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:24.211740Z",
     "iopub.status.busy": "2025-02-27T16:47:24.211466Z",
     "iopub.status.idle": "2025-02-27T16:47:25.014051Z",
     "shell.execute_reply": "2025-02-27T16:47:25.013037Z"
    },
    "id": "NriAECvS-AM6",
    "papermill": {
     "duration": 0.810205,
     "end_time": "2025-02-27T16:47:25.015465",
     "exception": false,
     "start_time": "2025-02-27T16:47:24.205260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 18811\n",
      "Number of validation images: 2566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "labels = [image.objects[0].category for image in anns]\n",
    "anns_train, anns_valid = train_test_split(anns, test_size=0.12, random_state=1, shuffle=True, stratify=labels)\n",
    "print('Number of training images: ' + str(len(anns_train)))\n",
    "print('Number of validation images: ' + str(len(anns_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d68fb757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:25.056806Z",
     "start_time": "2024-10-26T00:00:24.261581Z"
    },
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:25.027795Z",
     "iopub.status.busy": "2025-02-27T16:47:25.027319Z",
     "iopub.status.idle": "2025-02-27T16:47:26.741210Z",
     "shell.execute_reply": "2025-02-27T16:47:26.740315Z"
    },
    "id": "BNkjbY2e-AM7",
    "outputId": "47bde031-306f-464e-8e22-cc70a7fb7c67",
    "papermill": {
     "duration": 1.721456,
     "end_time": "2025-02-27T16:47:26.742516",
     "exception": false,
     "start_time": "2025-02-27T16:47:25.021060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model\n",
      "Number of devices: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150528</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │     <span style=\"color: #00af00; text-decoration-color: #00af00\">154,141,696</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,548</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150528\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │     \u001b[38;5;34m154,141,696\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m524,800\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)                  │           \u001b[38;5;34m1,548\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">154,832,268</span> (590.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m154,832,268\u001b[0m (590.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">154,832,268</span> (590.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m154,832,268\u001b[0m (590.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, LeakyReLU, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import HeNormal, GlorotUniform, Zeros\n",
    "\n",
    "print('Load model')\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(224, 224, 3)))  # Use Input layer instead of input_shape in Flatten\n",
    "    model.add(Flatten())\n",
    "    \n",
    "   \n",
    "    model.add(Dense(1024, activation='relu', kernel_initializer=HeNormal(), bias_initializer=Zeros()))\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer=HeNormal(), bias_initializer=Zeros()))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer=HeNormal(), bias_initializer=Zeros()))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer=HeNormal(), bias_initializer=Zeros()))\n",
    "    model.add(Dense(len(categories), kernel_initializer=GlorotUniform(),bias_initializer=Zeros()))\n",
    "    model.add(Activation('softmax'))\n",
    "   # Learning rate is changed to 0.001\n",
    "    opt = Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy() # Strategy for multi gpu usage\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "# buiding a model inside the strategy scope\n",
    "with strategy.scope():\n",
    "    multi_gpu_model = build_model()\n",
    "    \n",
    "multi_gpu_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca69f218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:26.254555Z",
     "start_time": "2024-10-26T00:00:26.243908Z"
    },
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:26.755622Z",
     "iopub.status.busy": "2025-02-27T16:47:26.755391Z",
     "iopub.status.idle": "2025-02-27T16:47:26.761478Z",
     "shell.execute_reply": "2025-02-27T16:47:26.760823Z"
    },
    "id": "GGAJEfpB-AM8",
    "papermill": {
     "duration": 0.013831,
     "end_time": "2025-02-27T16:47:26.762606",
     "exception": false,
     "start_time": "2025-02-27T16:47:26.748775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Callbacks\n",
    "model_checkpoint = ModelCheckpoint('model.keras', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau('val_accuracy', factor=0.1, patience=10, verbose=1)\n",
    "early_stop = EarlyStopping('val_accuracy', patience=40, verbose=1)\n",
    "terminate = TerminateOnNaN()\n",
    "callbacks = [model_checkpoint, reduce_lr, early_stop, terminate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6eccf88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:26.775494Z",
     "iopub.status.busy": "2025-02-27T16:47:26.775257Z",
     "iopub.status.idle": "2025-02-27T16:47:26.800090Z",
     "shell.execute_reply": "2025-02-27T16:47:26.799253Z"
    },
    "papermill": {
     "duration": 0.032785,
     "end_time": "2025-02-27T16:47:26.801460",
     "exception": false,
     "start_time": "2025-02-27T16:47:26.768675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Generate the list of objects from annotations\n",
    "objs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\n",
    "objs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n",
    "# Compute mean & std from training images before defining the generator\n",
    "#train_images = np.array([load_geoimage(filename) for filename, _ in objs_train])\n",
    "mean = [69.31671926,57.07645186,49.67770869]\n",
    "#np.mean(train_images, axis=(0,1,2))  # Compute mean per channel\n",
    "std = [51.84275139,44.94404335,41.23820328]#np.std(train_images, axis=(0,1,2))  # Compute std per channel\n",
    "\n",
    "def generator_images(objs, batch_size, do_shuffle=False):\n",
    "    while True:\n",
    "        if do_shuffle:\n",
    "            np.random.shuffle(objs)\n",
    "        groups = [objs[i:i+batch_size] for i in range(0, len(objs), batch_size)]\n",
    "        for group in groups:\n",
    "            images, labels = [], []\n",
    "            for (filename, obj) in group:\n",
    "                # Load image\n",
    "                img = load_geoimage(filename).astype(np.float32)\n",
    "\n",
    "                # Apply standardization\n",
    "                img = (img - mean) / std  \n",
    "\n",
    "                images.append(img)\n",
    "                probabilities = np.zeros(len(categories))\n",
    "                probabilities[list(categories.values()).index(obj.category)] = 1\n",
    "                labels.append(probabilities)\n",
    "            images = np.array(images).astype(np.float32)\n",
    "            labels = np.array(labels).astype(np.float32)\n",
    "            yield images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4959df48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:27.058834Z",
     "start_time": "2024-10-26T00:00:27.022627Z"
    },
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:26.813790Z",
     "iopub.status.busy": "2025-02-27T16:47:26.813584Z",
     "iopub.status.idle": "2025-02-27T16:47:26.816532Z",
     "shell.execute_reply": "2025-02-27T16:47:26.815968Z"
    },
    "id": "Yht-QqUH-AM8",
    "papermill": {
     "duration": 0.010372,
     "end_time": "2025-02-27T16:47:26.817678",
     "exception": false,
     "start_time": "2025-02-27T16:47:26.807306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generators\n",
    "batch_size = 128\n",
    "train_generator = generator_images(objs_train, batch_size, do_shuffle=True)\n",
    "valid_generator = generator_images(objs_valid, batch_size, do_shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62671121",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:26.830144Z",
     "iopub.status.busy": "2025-02-27T16:47:26.829933Z",
     "iopub.status.idle": "2025-02-27T16:47:26.836267Z",
     "shell.execute_reply": "2025-02-27T16:47:26.835471Z"
    },
    "papermill": {
     "duration": 0.013782,
     "end_time": "2025-02-27T16:47:26.837480",
     "exception": false,
     "start_time": "2025-02-27T16:47:26.823698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 98190925001567774\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 16266690560\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 6496594820407687368\n",
       " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3beea2d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-26T00:00:27.913670Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-02-27T16:47:26.850074Z",
     "iopub.status.busy": "2025-02-27T16:47:26.849817Z",
     "iopub.status.idle": "2025-02-27T17:43:27.481358Z",
     "shell.execute_reply": "2025-02-27T17:43:27.480346Z"
    },
    "id": "TrfpdECs-AM9",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "21d89b78-d94c-442e-9bc2-517654c0b614",
    "papermill": {
     "duration": 3360.63924,
     "end_time": "2025-02-27T17:43:27.482719",
     "exception": false,
     "start_time": "2025-02-27T16:47:26.843479",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Epoch 1/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.2103 - loss: 54.1424\n",
      "Epoch 1: val_accuracy improved from -inf to 0.31216, saving model to model.keras\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m815s\u001b[0m 6s/step - accuracy: 0.2105 - loss: 53.9414 - val_accuracy: 0.3122 - val_loss: 3.2789 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793ms/step - accuracy: 0.3516 - loss: 2.6002\n",
      "Epoch 2: val_accuracy improved from 0.31216 to 0.39478, saving model to model.keras\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 984ms/step - accuracy: 0.3517 - loss: 2.5978 - val_accuracy: 0.3948 - val_loss: 1.9323 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770ms/step - accuracy: 0.4412 - loss: 1.7004\n",
      "Epoch 3: val_accuracy improved from 0.39478 to 0.49532, saving model to model.keras\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 963ms/step - accuracy: 0.4413 - loss: 1.6999 - val_accuracy: 0.4953 - val_loss: 1.5388 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780ms/step - accuracy: 0.5250 - loss: 1.4401\n",
      "Epoch 4: val_accuracy improved from 0.49532 to 0.51520, saving model to model.keras\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 957ms/step - accuracy: 0.5249 - loss: 1.4401 - val_accuracy: 0.5152 - val_loss: 1.4806 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752ms/step - accuracy: 0.5620 - loss: 1.2871\n",
      "Epoch 5: val_accuracy improved from 0.51520 to 0.53780, saving model to model.keras\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 887ms/step - accuracy: 0.5619 - loss: 1.2872 - val_accuracy: 0.5378 - val_loss: 1.4723 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776ms/step - accuracy: 0.5869 - loss: 1.2046\n",
      "Epoch 6: val_accuracy did not improve from 0.53780\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 781ms/step - accuracy: 0.5869 - loss: 1.2048 - val_accuracy: 0.5327 - val_loss: 1.4675 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851ms/step - accuracy: 0.6109 - loss: 1.1385\n",
      "Epoch 7: val_accuracy improved from 0.53780 to 0.54716, saving model to model.keras\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 955ms/step - accuracy: 0.6109 - loss: 1.1386 - val_accuracy: 0.5472 - val_loss: 1.4159 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778ms/step - accuracy: 0.6377 - loss: 1.0613\n",
      "Epoch 8: val_accuracy did not improve from 0.54716\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 783ms/step - accuracy: 0.6377 - loss: 1.0615 - val_accuracy: 0.5394 - val_loss: 1.4954 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849ms/step - accuracy: 0.6562 - loss: 1.0225\n",
      "Epoch 9: val_accuracy improved from 0.54716 to 0.54988, saving model to model.keras\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 955ms/step - accuracy: 0.6562 - loss: 1.0225 - val_accuracy: 0.5499 - val_loss: 1.5209 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777ms/step - accuracy: 0.6776 - loss: 0.9430\n",
      "Epoch 10: val_accuracy did not improve from 0.54988\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 783ms/step - accuracy: 0.6775 - loss: 0.9432 - val_accuracy: 0.5417 - val_loss: 1.5389 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 839ms/step - accuracy: 0.6966 - loss: 0.8877\n",
      "Epoch 11: val_accuracy improved from 0.54988 to 0.55534, saving model to model.keras\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 945ms/step - accuracy: 0.6966 - loss: 0.8878 - val_accuracy: 0.5553 - val_loss: 1.4962 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933ms/step - accuracy: 0.7163 - loss: 0.8325\n",
      "Epoch 12: val_accuracy improved from 0.55534 to 0.56196, saving model to model.keras\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 1s/step - accuracy: 0.7162 - loss: 0.8326 - val_accuracy: 0.5620 - val_loss: 1.5194 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965ms/step - accuracy: 0.7398 - loss: 0.7610\n",
      "Epoch 13: val_accuracy did not improve from 0.56196\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 971ms/step - accuracy: 0.7397 - loss: 0.7613 - val_accuracy: 0.5608 - val_loss: 1.5390 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845ms/step - accuracy: 0.7593 - loss: 0.7159\n",
      "Epoch 14: val_accuracy improved from 0.56196 to 0.57132, saving model to model.keras\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 952ms/step - accuracy: 0.7592 - loss: 0.7162 - val_accuracy: 0.5713 - val_loss: 1.5996 - learning_rate: 0.0010\n",
      "Epoch 15/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924ms/step - accuracy: 0.7588 - loss: 0.7004\n",
      "Epoch 15: val_accuracy improved from 0.57132 to 0.58769, saving model to model.keras\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 1s/step - accuracy: 0.7588 - loss: 0.7004 - val_accuracy: 0.5877 - val_loss: 1.6436 - learning_rate: 0.0010\n",
      "Epoch 16/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913ms/step - accuracy: 0.7709 - loss: 0.6706\n",
      "Epoch 16: val_accuracy did not improve from 0.58769\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 918ms/step - accuracy: 0.7709 - loss: 0.6708 - val_accuracy: 0.5635 - val_loss: 1.6704 - learning_rate: 0.0010\n",
      "Epoch 17/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848ms/step - accuracy: 0.7809 - loss: 0.6632\n",
      "Epoch 17: val_accuracy did not improve from 0.58769\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 853ms/step - accuracy: 0.7808 - loss: 0.6633 - val_accuracy: 0.5651 - val_loss: 1.6939 - learning_rate: 0.0010\n",
      "Epoch 18/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842ms/step - accuracy: 0.7989 - loss: 0.5907\n",
      "Epoch 18: val_accuracy did not improve from 0.58769\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 848ms/step - accuracy: 0.7989 - loss: 0.5909 - val_accuracy: 0.5592 - val_loss: 1.8089 - learning_rate: 0.0010\n",
      "Epoch 19/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835ms/step - accuracy: 0.8131 - loss: 0.5570\n",
      "Epoch 19: val_accuracy did not improve from 0.58769\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 841ms/step - accuracy: 0.8131 - loss: 0.5571 - val_accuracy: 0.5674 - val_loss: 1.8505 - learning_rate: 0.0010\n",
      "Epoch 20/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844ms/step - accuracy: 0.8202 - loss: 0.5401\n",
      "Epoch 20: val_accuracy did not improve from 0.58769\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 849ms/step - accuracy: 0.8201 - loss: 0.5402 - val_accuracy: 0.5698 - val_loss: 1.8821 - learning_rate: 0.0010\n",
      "Best validation model: epoch 15  - val_accuracy 0.5876851081848145\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "print('Training model')\n",
    "epochs = 20\n",
    "train_steps = math.ceil(len(objs_train)/batch_size)\n",
    "valid_steps = math.ceil(len(objs_valid)/batch_size)\n",
    "\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    h = multi_gpu_model.fit(train_generator, steps_per_epoch=train_steps, validation_data=valid_generator, validation_steps=valid_steps, epochs=epochs, callbacks=callbacks, verbose=1)\n",
    "# Best validation model\n",
    "best_idx = int(np.argmax(h.history['val_accuracy']))\n",
    "best_value = np.max(h.history['val_accuracy'])\n",
    "print('Best validation model: epoch ' + str(best_idx+1), ' - val_accuracy ' + str(best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbcef7",
   "metadata": {
    "editable": true,
    "id": "8IMMO_mT-AM9",
    "papermill": {
     "duration": 0.142664,
     "end_time": "2025-02-27T17:43:27.773648",
     "exception": false,
     "start_time": "2025-02-27T17:43:27.630984",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Validation\n",
    "Compute validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed463854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:43:28.067711Z",
     "iopub.status.busy": "2025-02-27T17:43:28.067401Z",
     "iopub.status.idle": "2025-02-27T17:43:28.074555Z",
     "shell.execute_reply": "2025-02-27T17:43:28.073878Z"
    },
    "id": "HAanJ-V0-AM1",
    "papermill": {
     "duration": 0.156488,
     "end_time": "2025-02-27T17:43:28.075863",
     "exception": false,
     "start_time": "2025-02-27T17:43:27.919375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def draw_confusion_matrix(cm, categories):\n",
    "    # Draw confusion matrix\n",
    "    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n",
    "    ax = fig.add_subplot(111)\n",
    "    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap('Blues'))\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=list(categories.values()), yticklabels=list(categories.values()), ylabel='Annotation', xlabel='Prediction')\n",
    "    # Rotate the tick labels and set their alignment\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # Loop over data dimensions and create text annotations\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"/kaggle/working/confusion_matrix.png\", format=\"png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "354d5816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:43:28.362959Z",
     "iopub.status.busy": "2025-02-27T17:43:28.362597Z",
     "iopub.status.idle": "2025-02-27T17:49:07.942263Z",
     "shell.execute_reply": "2025-02-27T17:49:07.941394Z"
    },
    "papermill": {
     "duration": 339.726206,
     "end_time": "2025-02-27T17:49:07.943865",
     "exception": false,
     "start_time": "2025-02-27T17:43:28.217659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model = multi_gpu_model\n",
    "multi_gpu_model.load_weights('model.keras')\n",
    "y_true, y_pred = [], []\n",
    "for ann in anns_valid:\n",
    "    # Load image\n",
    "    image = load_geoimage(ann.filename).astype(np.float32)\n",
    "    image = (image - mean) / std \n",
    "    for obj_pred in ann.objects:\n",
    "        # Generate prediction\n",
    "        warped_image = np.expand_dims(image, 0)\n",
    "        predictions = model.predict(warped_image, verbose=0)\n",
    "        # Save prediction\n",
    "        pred_category = list(categories.values())[np.argmax(predictions)]\n",
    "        pred_score = np.max(predictions)\n",
    "        y_true.append(obj_pred.category)\n",
    "        y_pred.append(pred_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f439f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:49:08.294059Z",
     "iopub.status.busy": "2025-02-27T17:49:08.293689Z",
     "iopub.status.idle": "2025-02-27T17:49:10.374093Z",
     "shell.execute_reply": "2025-02-27T17:49:10.373346Z"
    },
    "papermill": {
     "duration": 2.227489,
     "end_time": "2025-02-27T17:49:10.375720",
     "exception": false,
     "start_time": "2025-02-27T17:49:08.148231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-dedca14cfdc5>:9: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap('Blues'))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))\n",
    "draw_confusion_matrix(cm, categories)\n",
    "results = h.history\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 10))\n",
    "\n",
    "# Plot Accuracy\n",
    "axes[0].plot(results[\"accuracy\"], label=\"Training Accuracy\", color=\"blue\")\n",
    "axes[0].plot(results[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"green\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_title(\"Accuracy Over Epochs\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "axes[0].set_ylim(0, 1)  # Normalize y-axis for accuracy (0-1 range)\n",
    "\n",
    "# Plot Loss\n",
    "axes[1].plot(results[\"loss\"], label=\"Training Loss\", color=\"red\")\n",
    "axes[1].plot(results[\"val_loss\"], label=\"Validation Loss\", color=\"orange\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].set_title(\"Loss Over Epochs\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "axes[1].set_ylim(0, max(max(results[\"loss\"]), max(results[\"val_loss\"])) * 1.1)  # Normalize loss y-axis\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"/kaggle/working/plot.png\", format=\"png\", bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "919c53ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:49:10.667663Z",
     "iopub.status.busy": "2025-02-27T17:49:10.667349Z",
     "iopub.status.idle": "2025-02-27T17:49:10.677933Z",
     "shell.execute_reply": "2025-02-27T17:49:10.677051Z"
    },
    "papermill": {
     "duration": 0.155275,
     "end_time": "2025-02-27T17:49:10.679244",
     "exception": false,
     "start_time": "2025-02-27T17:49:10.523969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 58.769%\n",
      "Mean Recall: 48.415%\n",
      "Mean Precision: 53.368%\n",
      "> Cargo plane: Recall: 67.105% Precision: 87.931% Specificity: 99.719% Dice: 76.119%\n",
      "> Helicopter: Recall: 0.000% Precision: 0.000% Specificity: 100.000% Dice: 0.000%\n",
      "> Small car: Recall: 87.961% Precision: 63.092% Specificity: 87.079% Dice: 73.479%\n",
      "> Bus: Recall: 33.591% Precision: 50.289% Specificity: 96.272% Dice: 40.278%\n",
      "> Truck: Recall: 28.485% Precision: 34.559% Specificity: 92.039% Dice: 31.229%\n",
      "> Motorboat: Recall: 43.750% Precision: 48.696% Specificity: 97.580% Dice: 46.091%\n",
      "> Fishing vessel: Recall: 43.529% Precision: 59.677% Specificity: 98.992% Dice: 50.340%\n",
      "> Dump truck: Recall: 42.568% Precision: 53.846% Specificity: 97.767% Dice: 47.547%\n",
      "> Excavator: Recall: 54.737% Precision: 62.651% Specificity: 98.745% Dice: 58.427%\n",
      "> Building: Recall: 76.377% Precision: 70.033% Specificity: 90.814% Dice: 73.067%\n",
      "> Storage tank: Recall: 46.591% Precision: 64.062% Specificity: 98.075% Dice: 53.947%\n",
      "> Shipping container: Recall: 56.284% Precision: 45.575% Specificity: 94.838% Dice: 50.367%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute the accuracy\n",
    "correct_samples_class = np.diag(cm).astype(float)\n",
    "total_samples_class = np.sum(cm, axis=1).astype(float)\n",
    "total_predicts_class = np.sum(cm, axis=0).astype(float)\n",
    "print('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\n",
    "acc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\n",
    "print('Mean Recall: %.3f%%' % (acc.mean() * 100))\n",
    "acc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\n",
    "print('Mean Precision: %.3f%%' % (acc.mean() * 100))\n",
    "for idx in range(len(categories)):\n",
    "    # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n",
    "    # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n",
    "    tp = cm[idx, idx]\n",
    "    fp = sum(cm[:, idx]) - tp\n",
    "    fn = sum(cm[idx, :]) - tp\n",
    "    tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n",
    "    # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n",
    "    recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n",
    "    # Precision: proportion of predicted positive cases that were truly real positives.\n",
    "    precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n",
    "    # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n",
    "    specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n",
    "    # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n",
    "    # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n",
    "    f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n",
    "    print('> %s: Recall: %.3f%% Precision: %.3f%% Specificity: %.3f%% Dice: %.3f%%' % (list(categories.values())[idx], recall*100, precision*100, specificity*100, f1_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84029c",
   "metadata": {
    "papermill": {
     "duration": 0.142161,
     "end_time": "2025-02-27T17:49:10.964267",
     "exception": false,
     "start_time": "2025-02-27T17:49:10.822106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Testing\n",
    "Try to improve the results provided in the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04d43b7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:49:11.248771Z",
     "iopub.status.busy": "2025-02-27T17:49:11.248472Z",
     "iopub.status.idle": "2025-02-27T17:49:15.062325Z",
     "shell.execute_reply": "2025-02-27T17:49:15.061243Z"
    },
    "id": "tJr_-xCt-AM-",
    "papermill": {
     "duration": 3.95835,
     "end_time": "2025-02-27T17:49:15.063705",
     "exception": false,
     "start_time": "2025-02-27T17:49:11.105355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing images: 2635\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "anns = []\n",
    "for (dirpath, dirnames, filenames) in os.walk('/kaggle/input/deeplearning/xview_recognition/xview_test'):\n",
    "    for filename in filenames:\n",
    "        image = GenericImage('xview_test/'+'/'+filename)\n",
    "        image.tile = np.array([0, 0, 224, 224])\n",
    "        obj = GenericObject()\n",
    "        obj.bb = (0, 0, 224, 224)\n",
    "        obj.category = dirpath[dirpath.rfind('/')+1:]\n",
    "        image.add_object(obj)\n",
    "        anns.append(image)\n",
    "print('Number of testing images: ' + str(len(anns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c13659a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:49:15.348053Z",
     "iopub.status.busy": "2025-02-27T17:49:15.347700Z",
     "iopub.status.idle": "2025-02-27T17:56:39.056388Z",
     "shell.execute_reply": "2025-02-27T17:56:39.055608Z"
    },
    "id": "TGs2zqfv-AM_",
    "papermill": {
     "duration": 443.850876,
     "end_time": "2025-02-27T17:56:39.058111",
     "exception": false,
     "start_time": "2025-02-27T17:49:15.207235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model.load_weights('model.keras')\n",
    "predictions_data = {\"images\": {}, \"annotations\": {}}\n",
    "for idx, ann in enumerate(anns):\n",
    "    image_data = {\"image_id\": ann.filename.split('/')[-1], \"filename\": ann.filename, \"width\": int(ann.tile[2]), \"height\": int(ann.tile[3])}\n",
    "    predictions_data[\"images\"][idx] = image_data\n",
    "    # Load image\n",
    "    image = load_geoimage(ann.filename).astype(np.float32)\n",
    "    image = (image - mean) / std\n",
    "    for obj_pred in ann.objects:\n",
    "        # Generate prediction\n",
    "        warped_image = np.expand_dims(image, 0)\n",
    "        predictions = model.predict(warped_image, verbose=0)\n",
    "        # Save prediction\n",
    "        pred_category = list(categories.values())[np.argmax(predictions)]\n",
    "        pred_score = np.max(predictions)\n",
    "        annotation_data = {\"image_id\": ann.filename.split('/')[-1], \"category_id\": pred_category, \"bbox\": [int(x) for x in obj_pred.bb]}\n",
    "        predictions_data[\"annotations\"][idx] = annotation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e958a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:56:39.401084Z",
     "iopub.status.busy": "2025-02-27T17:56:39.400752Z",
     "iopub.status.idle": "2025-02-27T17:56:39.449654Z",
     "shell.execute_reply": "2025-02-27T17:56:39.449041Z"
    },
    "papermill": {
     "duration": 0.194408,
     "end_time": "2025-02-27T17:56:39.450740",
     "exception": false,
     "start_time": "2025-02-27T17:56:39.256332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/prediction.json\", \"w\") as outfile:\n",
    "    json.dump(predictions_data, outfile)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6701413,
     "sourceId": 10797536,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6708542,
     "sourceId": 10807452,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4180.535208,
   "end_time": "2025-02-27T17:56:42.708417",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-27T16:47:02.173209",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
